---
title: Accepted Papers
nav: true
---

We are happy to announce the following papers have been accepted at the workshop:

## AI Copilots for Reproducibility in Science: A Case Study
_Adrien Bibal, Steven Minton, Deborah Khider, Yolanda Gil_

Link: <https://arxiv.org/pdf/2506.20130>

## AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents
_Bhanu Prakash Vangala, Ali Adibifar, Tanu Malik, Ashish Gehani_

Link: <https://www.arxiv.org/pdf/2512.22387>

## Automated Reproducibility Has a Problem Statement Problem
_Thijs Snelleman, Peter Lundestad Lawrence, Holger H. Hoos, Odd Erik Gundersen_

Link: <https://arxiv.org/pdf/2601.04226>

## CAIBench: A Meta-Benchmark for Reproducible Labor-Relevant Agentic Cybersecurity Tasks
_María Sanz-Gómez, Víctor Mayoral-Vilches, Francesco Balassone, Luis Javier Navarrete Lozano, Cristobal Ricardo Jesús Veas Chavez, Maite del Mundo de Torres_

Link: <https://arxiv.org/pdf/2510.24317>

## Exploration of Reproducible Generated Image Detection
_Yihang Duan_

Link: <https://arxiv.org/pdf/2512.21562>

## Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context
_Anatole Jacquin de Margerie, Alexis Roger_

Link: <https://arxiv.org/pdf/2512.11167>

## open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison
_Marianna Nezhurina, Jörg K.H. Franke, Taishi Nakamura, Timur Carstensen, Niccolò Ajroldi, Ville Komulainen, David Salinas, Jenia Jitsev_

Link: <https://arxiv.org/pdf/2509.09009>

## Learning to be Reproducible: Custom Loss Design for Robust Neural Networks
_Waqas Ahmed, Sheeba Samuel, Kevin Coakley, Birgitta Koenig-Ries, Odd Erik Gundersen_

Link: <https://arxiv.org/pdf/2601.00578>

## Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology
_Vanessa D'Amario, Randy Daniel, Alessandro Zanetti, Dhruv Edamadaka, Nitya Alaparthy, Joshua Tarkoff_

Link: <https://www.arxiv.org/pdf/2601.11567>

## Simpler Methods Work Better for L1 Penalized Logistic Models and Large Datasets
_Edward Raff, James Holt_

Link: TBA
